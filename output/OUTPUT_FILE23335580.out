Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23335580: <testjob_ddpm_cifar> in cluster <dcc> Done

Job <testjob_ddpm_cifar> was submitted from host <gbarlogin1> by user <s233670> in cluster <dcc> at Wed Dec  4 00:41:33 2024
Job was executed on host(s) <4*n-62-20-11>, in queue <gpuv100>, as user <s233670> in cluster <dcc> at Wed Dec  4 00:44:41 2024
</zhome/d8/f/203934> was used as the home directory.
</zhome/d8/f/203934/ddpm/deep_learning_ddpm_group_7> was used as the working directory.
Started at Wed Dec  4 00:44:41 2024
Terminated at Wed Dec  4 02:28:12 2024
Results reported at Wed Dec  4 02:28:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
### ------------- specify queue name ---------------- 
#BSUB -q gpuv100

### ------------- specify gpu request---------------- 
#BSUB -gpu "num=1:mode=exclusive_process"

### ------------- specify job name ---------------- 
#BSUB -J testjob_ddpm_cifar

### ------------- specify number of cores ---------------- 
#BSUB -n 4 
#BSUB -R "span[hosts=1]"

### ------------- specify CPU memory requirements ---------------- 
#BSUB -R "rusage[mem=30GB]"

#BSUB -W 12:00 
#BSUB -o output/OUTPUT_FILE%J.out 
#BSUB -e output/OUTPUT_FILE%J.err

source "/zhome/d8/f/203934/ddpm/deep_learning_ddpm_group_7/.venv/bin/activate"
python -m tools.train_ddpm
python -m tools.sample_ddpm
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   7263.00 sec.
    Max Memory :                                 3582 MB
    Average Memory :                             3285.95 MB
    Total Requested Memory :                     122880.00 MB
    Delta Memory :                               119298.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   6212 sec.
    Turnaround time :                            6399 sec.

The output (if any) follows:

cuda
{'dataset_params': {'im_path': 'data/train/images'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 3, 'im_size': 32, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 128], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'num_heads': 4}, 'train_params': {'task_name': 'default_fid', 'batch_size': 128, 'num_epochs': 30, 'num_samples': 100, 'num_grid_rows': 10, 'lr': 0.0002, 'ckpt_name': 'ddpm_ckpt_cifar.pth'}}
Files already downloaded and verified
Files already downloaded and verified
<torch.utils.data.dataloader.DataLoader object at 0x7f5176aff910>
Finished epoch:1 | Loss : 0.1303
Finished epoch:2 | Loss : 0.0426
Finished epoch:3 | Loss : 0.0386
Finished epoch:4 | Loss : 0.0368
Finished epoch:5 | Loss : 0.0365
Finished epoch:6 | Loss : 0.0353
Finished epoch:7 | Loss : 0.0351
Finished epoch:8 | Loss : 0.0347
Finished epoch:9 | Loss : 0.0350
Finished epoch:10 | Loss : 0.0341
Finished epoch:11 | Loss : 0.0340
Finished epoch:12 | Loss : 0.0340
Finished epoch:13 | Loss : 0.0330
Finished epoch:14 | Loss : 0.0331
Finished epoch:15 | Loss : 0.0329
Finished epoch:16 | Loss : 0.0335
Finished epoch:17 | Loss : 0.0327
Finished epoch:18 | Loss : 0.0329
Finished epoch:19 | Loss : 0.0328
Finished epoch:20 | Loss : 0.0326
Finished epoch:21 | Loss : 0.0330
Finished epoch:22 | Loss : 0.0325
Finished epoch:23 | Loss : 0.0324
Finished epoch:24 | Loss : 0.0328
Finished epoch:25 | Loss : 0.0327
Finished epoch:26 | Loss : 0.0320
Finished epoch:27 | Loss : 0.0324
Finished epoch:28 | Loss : 0.0320
Finished epoch:29 | Loss : 0.0323
Finished epoch:30 | Loss : 0.0322
Done Training ...
{'dataset_params': {'im_path': 'data/train/images'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 3, 'im_size': 32, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 128], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'num_heads': 4}, 'train_params': {'task_name': 'default_fid', 'batch_size': 128, 'num_epochs': 30, 'num_samples': 100, 'num_grid_rows': 10, 'lr': 0.0002, 'ckpt_name': 'ddpm_ckpt_cifar.pth'}}


PS:

Read file <output/OUTPUT_FILE23335580.err> for stderr output of this job.

